---
title: "Let's Talk About Risk (And How to Manage It)"
subject: "Safety, compliance, and clinical responsibility"
newsletter_number: 11
---

# Let's Talk About Risk (And How to Manage It)

You've learned the techniques. You're saving time. But there's a question we need to address head-on:

**How do we use AI-assisted documentation safely and responsibly?**

This isn't theoretical. These are practical guidelines for maintaining clinical responsibility while leveraging automation.

## The Non-Negotiable Rule

**You review every note before signing.**

I don't care if the AI has been 100% accurate for six months straight. You review. You approve. You're responsible.

This is the same standard you apply to any documentation tool—whether it's an AI scribe, voice recognition, or a medical student's draft.

The tool assists. You decide.

## The Three-Layer Safety Framework

**Layer 1: Institutional Approval**
Before implementing any AI tool, confirm it's approved by your institution's IT and compliance teams.

Most EMR-integrated AI features are already vetted. External tools (ChatGPT, Claude, etc.) may require additional review.

Ask your compliance officer. Document the approval.

**Layer 2: HIPAA Compliance**
Use tools that are HIPAA-compliant or don't process PHI (protected health information).

Our browser-based tools (Snippet Manager, E&M Calculator) never transmit data—they run locally. External AI services may require BAAs (Business Associate Agreements).

When in doubt, de-identify data before processing.

**Layer 3: Documented Review**
Your EMR should show you reviewed and approved AI-generated content. Most systems timestamp edits and approvals.

If audited, you need to demonstrate physician oversight at every step.

## Common Scenarios & How to Handle Them

**Scenario 1: AI suggests a diagnosis you didn't consider**
→ Review carefully. If valid, great. If not, delete it. The AI formatted scribe output—it didn't make clinical decisions.

**Scenario 2: Medication list is incomplete**
→ The AI can only format what the scribe captured. You add missing information manually. This is normal.

**Scenario 3: Prompt produces inconsistent results**
→ Refine your examples. Inconsistency usually means the prompt needs better training data.

**Scenario 4: You notice an error after signing**
→ Addendum/correction, same as any documentation error. Not unique to AI.

## What About Malpractice?

Real talk: Using AI-assisted documentation doesn't increase malpractice risk if you're reviewing output.

The risk is signing notes without reading them—whether they're written by AI, a scribe, or a resident.

The standard is the same: The attending physician reviews and approves all clinical documentation.

## Patient Consent

Some institutions require disclosure that AI was used in documentation. Check your local requirements.

Most patients don't care how notes are generated. They care about quality of care.

If asked: "I use AI to help format my notes more efficiently, which gives me more time to focus on your care. I review everything before finalizing."

## Getting Institutional Buy-In

Want to implement this across your practice/group? Here's the pitch:

**To administrators:**
- Reduces physician burnout (documented time savings)
- Improves billing accuracy (E&M Calculator)
- No additional software costs (uses existing EMR AI features)
- Maintains clinical oversight standards

**To compliance:**
- HIPAA-compliant workflow (use approved tools only)
- Documented physician review at every step
- Follows same QA protocols as current documentation

**To IT:**
- Leverages existing EMR capabilities (no new systems)
- Browser-based tools require no installation
- Can start with pilot group before broader rollout

**Resources for implementation:**
[Download Implementation Guide →]({{ site.baseurl }}/best-practices#institutional-implementation)

## The Bottom Line

AI-assisted documentation is a tool, not a shortcut around clinical responsibility.

Use it the same way you'd use any documentation assistant: with oversight, review, and professional judgment.

That's not just safe—it's the standard of care.

## Next Week: The Final Newsletter

I'll share the long-term strategy: Building a comprehensive prompt library, staying current with AI advances, and contributing to the community.

Plus, what's next for Physician Prompt Engineering (hint: new tools, expanded course content, and some exciting community features).

Until then,

PedsCoffee

P.S. If you have specific compliance questions, reply to this email. I'll compile common questions into a FAQ resource for the community.
