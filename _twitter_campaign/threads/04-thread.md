---
type: thread
week: 2
topic: Privacy-first AI tools
hashtags: [#MedTwitter, #HealthIT, #HIPAA]
tweets_count: 9
---

TWEET 1/9:
Using AI for clinical documentation without violating HIPAA requires intentional practices. Here's a practical privacy framework. ðŸ§µ

---

TWEET 2/9:
Rule 1: Never use patient identifiers. No names, DOBs, MRNs, addresses, phone numbers, or specific dates. Use "68-year-old" not "John Smith, DOB 3/15/1955."

---

TWEET 3/9:
Rule 2: De-identify locations. "Local hospital" not "Stanford Medical Center." "Patient's workplace" not "Google headquarters."

---

TWEET 4/9:
Rule 3: Remove rare conditions or unique circumstances that could identify patients. Use clinical judgment about what's identifying.

---

TWEET 5/9:
Rule 4: Prefer tools with Business Associate Agreements when available. Consumer AI tools like ChatGPT free tier = not HIPAA compliant.

---

TWEET 6/9:
Rule 5: Use local/browser-based tools when possible. Processing data client-side is inherently more private than server-based processing.

---

TWEET 7/9:
Rule 6: Clear chat histories regularly. Even de-identified data should follow minimal retention principles.

---

TWEET 8/9:
Rule 7: Know your institution's AI policy. Some require additional approvals, patient consent, or specific tool usage.

---

TWEET 9/9:
Privacy isn't one-and-done. It's a habit in every interaction. When in doubt, remove more identifying information, not less.
