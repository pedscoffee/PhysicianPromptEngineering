---
type: thread
week: 6
topic: Handling hallucinations
hashtags: [#MedTwitter, #ClinicalAI, #PatientSafety]
tweets_count: 8
---

TWEET 1/8:
AI hallucinations in medical documentation are dangerous. Here's how to catch them before they reach the chart. ðŸ§µ

---

TWEET 2/8:
Hallucination = AI generating plausible-sounding information not present in your input. In medicine, this is unacceptable.

---

TWEET 3/8:
Common hallucinations: Vitals you didn't provide, medications patient isn't taking, lab values you didn't mention, diagnoses not discussed.

---

TWEET 4/8:
Prevention strategy 1: Explicit constraints. "Only include information explicitly provided. Do not infer or extrapolate clinical data."

---

TWEET 5/8:
Prevention strategy 2: Structured input. Provide information in consistent format. AI is less likely to hallucinate with structured data.

---

TWEET 6/8:
Detection strategy: Always review AI output line by line. Check vitals, medications, allergies, diagnoses against your source data.

---

TWEET 7/8:
Red flags: Oddly specific details you don't remember providing, perfectly normal values when patient is clearly ill, medications you don't use.

---

TWEET 8/8:
You are the physician. AI is the tool. Every word in the chart is your responsibility. Review everything, every time.
